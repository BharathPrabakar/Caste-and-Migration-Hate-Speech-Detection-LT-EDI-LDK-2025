{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Pa2e_qMgsN"
      },
      "source": [
        "CNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VzLO__QRXND"
      },
      "source": [
        "1.CNN-based model for text classification that uses your train.csv, test.csv, and dev.csv datasets. The code preprocesses the data, tokenizes the text, builds a CNN model using TensorFlow/Keras, trains the model, and generates predictions on the test set.\n",
        "\n",
        "This script:\n",
        "\n",
        "Loads the datasets from CSV files.\n",
        "\n",
        "Tokenizes and pads the text data.\n",
        "\n",
        "Builds a CNN model for text classification.\n",
        "\n",
        "Trains the model using the training data.\n",
        "\n",
        "Predicts labels for the test dataset and saves them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "le-iuqZmL_w5",
        "outputId": "1feb62f4-5d4a-43c1-9540-6eb8bf38aa2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 50ms/step - accuracy: 0.6024 - loss: 0.6685 - val_accuracy: 0.6760 - val_loss: 0.6036\n",
            "Epoch 2/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 45ms/step - accuracy: 0.8179 - loss: 0.4380 - val_accuracy: 0.7776 - val_loss: 0.4762\n",
            "Epoch 3/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - accuracy: 0.9570 - loss: 0.1358 - val_accuracy: 0.7980 - val_loss: 0.5991\n",
            "Epoch 4/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 67ms/step - accuracy: 0.9890 - loss: 0.0407 - val_accuracy: 0.7954 - val_loss: 0.6713\n",
            "Epoch 5/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.9928 - loss: 0.0242 - val_accuracy: 0.7865 - val_loss: 0.7091\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
            "Predictions saved to test_predictions_01.csv\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Classification Report on Dev Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.82      0.83       485\n",
            "           1       0.72      0.73      0.72       302\n",
            "\n",
            "    accuracy                           0.79       787\n",
            "   macro avg       0.77      0.77      0.77       787\n",
            "weighted avg       0.79      0.79      0.79       787\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "dev_df = pd.read_csv('/content/dev.csv')\n",
        "\n",
        "# Extract text and labels\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "train_labels = train_df['label'].values\n",
        "\n",
        "dev_texts = dev_df['text'].astype(str).tolist()\n",
        "dev_labels = dev_df['label'].values\n",
        "\n",
        "test_texts = test_df['text'].astype(str).tolist()\n",
        "test_ids = test_df['id']  # Extract test IDs\n",
        "\n",
        "# Tokenization\n",
        "max_words = 10000  # Vocabulary size\n",
        "max_len = 100  # Max length of sequences\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=max_len)\n",
        "X_dev = pad_sequences(tokenizer.texts_to_sequences(dev_texts), maxlen=max_len)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_texts), maxlen=max_len)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_dev = np.array(dev_labels)\n",
        "\n",
        "# Build CNN Model\n",
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=5, batch_size=32)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict(X_test)\n",
        "test_predictions = (test_predictions > 0.5).astype(int)  # Convert probabilities to binary labels\n",
        "\n",
        "# Save predictions (only 'id' and 'predicted_label')\n",
        "output_df = pd.DataFrame({'id': test_ids, 'predicted_label': test_predictions.flatten()})\n",
        "output_df.to_csv('/content/test_predictions_01.csv', index=False)\n",
        "print(\"Predictions saved to test_predictions_01.csv\")\n",
        "\n",
        "# Classification report on dev set\n",
        "y_dev_pred = (model.predict(X_dev) > 0.5).astype(int)\n",
        "print(\"Classification Report on Dev Set:\")\n",
        "print(classification_report(y_dev, y_dev_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_-Xmz4pSv5Z"
      },
      "source": [
        "2.code to support multi-class classification and added a classification report for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShTr06hiS2Nm",
        "outputId": "6fc2bf03-4158-4187-c587-5898bf0a57bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 52ms/step - accuracy: 0.6225 - loss: 0.6600 - val_accuracy: 0.7078 - val_loss: 0.5837\n",
            "Epoch 2/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 46ms/step - accuracy: 0.8320 - loss: 0.4029 - val_accuracy: 0.7967 - val_loss: 0.4531\n",
            "Epoch 3/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 45ms/step - accuracy: 0.9715 - loss: 0.1059 - val_accuracy: 0.7992 - val_loss: 0.5632\n",
            "Epoch 4/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 51ms/step - accuracy: 0.9907 - loss: 0.0389 - val_accuracy: 0.8030 - val_loss: 0.6478\n",
            "Epoch 5/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 50ms/step - accuracy: 0.9924 - loss: 0.0238 - val_accuracy: 0.7992 - val_loss: 0.7054\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
            "Predictions saved to test_predictions.csv\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84       485\n",
            "           1       0.76      0.69      0.73       302\n",
            "\n",
            "    accuracy                           0.80       787\n",
            "   macro avg       0.79      0.78      0.78       787\n",
            "weighted avg       0.80      0.80      0.80       787\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "dev_df = pd.read_csv('/content/dev.csv')\n",
        "\n",
        "# Assuming the dataset has 'text' and 'label' columns\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "train_labels = train_df['label'].astype(str).tolist()\n",
        "test_texts = test_df['text'].astype(str).tolist()\n",
        "dev_texts = dev_df['text'].astype(str).tolist()\n",
        "dev_labels = dev_df['label'].astype(str).tolist()\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_labels)\n",
        "y_dev = label_encoder.transform(dev_labels)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Tokenization\n",
        "max_words = 10000  # Vocabulary size\n",
        "max_len = 100  # Max length of sequences\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=max_len)\n",
        "X_dev = pad_sequences(tokenizer.texts_to_sequences(dev_texts), maxlen=max_len)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_texts), maxlen=max_len)\n",
        "\n",
        "# Build CNN Model\n",
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
        "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')  # Multi-class classification\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=5, batch_size=32)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict(X_test)\n",
        "test_pred_labels = np.argmax(test_predictions, axis=1)  # Convert probabilities to class labels\n",
        "\n",
        "# Save predictions\n",
        "output_df = pd.DataFrame({'id': test_ids, 'predicted_label': label_encoder.inverse_transform(test_pred_labels)})\n",
        "output_df.to_csv('/content/test_predictions_02.csv', index=False)\n",
        "print(\"Predictions saved to test_predictions.csv\")\n",
        "\n",
        "# Classification report\n",
        "y_dev_pred = np.argmax(model.predict(X_dev), axis=1)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_dev, y_dev_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bugSbHjrWbsV"
      },
      "source": [
        "3.the code to use a BERT model for multi-class text classification. It now includes BERT tokenization, model training, and a classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NkwJopFaFmm",
        "outputId": "de016897-2a93-4e50-9349-5ce95639a5ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 70ms/step - accuracy: 0.6157 - loss: 0.6683 - val_accuracy: 0.7116 - val_loss: 0.6214\n",
            "Epoch 2/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 63ms/step - accuracy: 0.8088 - loss: 0.4448 - val_accuracy: 0.7814 - val_loss: 0.4763\n",
            "Epoch 3/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - accuracy: 0.9720 - loss: 0.1192 - val_accuracy: 0.7738 - val_loss: 0.5826\n",
            "Epoch 4/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 65ms/step - accuracy: 0.9900 - loss: 0.0348 - val_accuracy: 0.7954 - val_loss: 0.7101\n",
            "Epoch 5/5\n",
            "\u001b[1m173/173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 67ms/step - accuracy: 0.9931 - loss: 0.0205 - val_accuracy: 0.7942 - val_loss: 0.8219\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
            "Predictions saved to test_predictions.csv\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84       485\n",
            "           1       0.77      0.66      0.71       302\n",
            "\n",
            "    accuracy                           0.79       787\n",
            "   macro avg       0.79      0.77      0.78       787\n",
            "weighted avg       0.79      0.79      0.79       787\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "dev_df = pd.read_csv('/content/dev.csv')\n",
        "\n",
        "# Assuming dataset has 'text' and 'label' columns\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "train_labels = train_df['label'].astype(str).tolist()\n",
        "test_texts = test_df['text'].astype(str).tolist()\n",
        "dev_texts = dev_df['text'].astype(str).tolist()\n",
        "dev_labels = dev_df['label'].astype(str).tolist()\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_labels)\n",
        "y_dev = label_encoder.transform(dev_labels)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_dev = to_categorical(y_dev, num_classes)\n",
        "\n",
        "# Tokenization and Padding\n",
        "max_words = 10000  # Maximum vocabulary size\n",
        "max_len = 100  # Maximum sequence length\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=max_len, padding='post', truncating='post')\n",
        "X_dev = pad_sequences(tokenizer.texts_to_sequences(dev_texts), maxlen=max_len, padding='post', truncating='post')\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_texts), maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# CNN Model for Text Classification\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train Model\n",
        "model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=5, batch_size=32)\n",
        "\n",
        "# Predict on test data\n",
        "test_predictions = model.predict(X_test)\n",
        "test_pred_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Save Predictions\n",
        "output_df = pd.DataFrame({'id': test_ids, 'predicted_label': label_encoder.inverse_transform(test_pred_labels)})\n",
        "output_df.to_csv('/content/test_predictions_03.csv', index=False)\n",
        "print(\"Predictions saved to test_predictions.csv\")\n",
        "\n",
        "# Classification Report\n",
        "y_dev_pred = np.argmax(model.predict(X_dev), axis=1)\n",
        "y_dev_true = np.argmax(y_dev, axis=1)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_dev_true, y_dev_pred, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQD4YVE6PQi1",
        "outputId": "a74eddaf-04b0-4000-ba4b-bc7c1d37af30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to test_predictions_svm.csv\n",
            "Classification Report on Dev Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.86      0.78       485\n",
            "           1       0.67      0.48      0.56       302\n",
            "\n",
            "    accuracy                           0.71       787\n",
            "   macro avg       0.70      0.67      0.67       787\n",
            "weighted avg       0.70      0.71      0.70       787\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "dev_df = pd.read_csv('/content/dev.csv')\n",
        "\n",
        "# Assuming dataset has 'text' and 'label' columns\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "train_labels = train_df['label'].astype(str).tolist()\n",
        "test_texts = test_df['text'].astype(str).tolist()\n",
        "dev_texts = dev_df['text'].astype(str).tolist()\n",
        "dev_labels = dev_df['label'].astype(str).tolist()\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_labels)\n",
        "y_dev = label_encoder.transform(dev_labels)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(train_texts)\n",
        "X_dev_tfidf = vectorizer.transform(dev_texts)\n",
        "X_test_tfidf = vectorizer.transform(test_texts)\n",
        "\n",
        "# SVM Classifier (using a linear kernel)\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict on dev and test data\n",
        "dev_predictions = svm_model.predict(X_dev_tfidf)\n",
        "test_predictions = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Save Predictions (Test Data)\n",
        "output_df = pd.DataFrame({'id': test_df['id'], 'predicted_label': label_encoder.inverse_transform(test_predictions)})\n",
        "output_df.to_csv('/content/test_predictions_svm.csv', index=False)\n",
        "print(\"Predictions saved to test_predictions_svm.csv\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"Classification Report on Dev Data:\")\n",
        "print(classification_report(y_dev, dev_predictions, target_names=label_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "38961e1214994b4689111b905fe1403b",
            "dd6731543dda455cb5a8fdc989dddc44",
            "e786c93896d74a1fa75f99fe64deb6d4",
            "8827799bee1848fa89fcdbcd1317e423",
            "e12076383acc45c09ae721425c328463"
          ]
        },
        "id": "D88tm5znRD7W",
        "outputId": "ab8ca393-31fa-4885-bbfd-8264c77369a0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38961e1214994b4689111b905fe1403b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd6731543dda455cb5a8fdc989dddc44",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e786c93896d74a1fa75f99fe64deb6d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8827799bee1848fa89fcdbcd1317e423",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e12076383acc45c09ae721425c328463",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "173/173 [==============================] - 195s 840ms/step - loss: 0.6575 - accuracy: 0.6138 - val_loss: 0.6410 - val_accuracy: 0.6163\n",
            "Epoch 2/3\n",
            "173/173 [==============================] - 144s 833ms/step - loss: 0.6136 - accuracy: 0.6480 - val_loss: 0.5884 - val_accuracy: 0.6811\n",
            "Epoch 3/3\n",
            "173/173 [==============================] - 144s 831ms/step - loss: 0.4817 - accuracy: 0.7640 - val_loss: 0.5884 - val_accuracy: 0.7154\n",
            "25/25 [==============================] - 10s 273ms/step\n",
            "Classification Report (Dev):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.89      0.79       485\n",
            "           1       0.71      0.44      0.54       302\n",
            "\n",
            "    accuracy                           0.72       787\n",
            "   macro avg       0.71      0.66      0.67       787\n",
            "weighted avg       0.71      0.72      0.70       787\n",
            "\n",
            "50/50 [==============================] - 17s 277ms/step\n",
            "Predictions saved to test_predictions_mbert.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "dev_df = pd.read_csv('/content/dev.csv')\n",
        "\n",
        "# Extract text and labels\n",
        "train_texts = train_df['text'].astype(str).tolist()\n",
        "train_labels = train_df['label'].astype(str).tolist()\n",
        "test_texts = test_df['text'].astype(str).tolist()\n",
        "dev_texts = dev_df['text'].astype(str).tolist()\n",
        "dev_labels = dev_df['label'].astype(str).tolist()\n",
        "\n",
        "# Encode labels (integer encoding)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_labels)\n",
        "y_dev = label_encoder.transform(dev_labels)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Tokenizer and model\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input texts\n",
        "def tokenize_data(texts, tokenizer, max_len=128):\n",
        "    return tokenizer(\n",
        "        texts,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_len,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_data(train_texts, tokenizer)\n",
        "dev_encodings = tokenize_data(dev_texts, tokenizer)\n",
        "test_encodings = tokenize_data(test_texts, tokenizer)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        ")).shuffle(1000).batch(32)\n",
        "\n",
        "dev_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(dev_encodings),\n",
        "    y_dev\n",
        ")).batch(32)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(dict(test_encodings)).batch(32)\n",
        "\n",
        "# Load model\n",
        "model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n",
        "\n",
        "# Optimizer setup\n",
        "epochs = 3\n",
        "batch_size = 32\n",
        "train_steps = len(train_dataset) * epochs\n",
        "optimizer, _ = create_optimizer(init_lr=2e-5, num_train_steps=train_steps, num_warmup_steps=0)\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "# Train model\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=dev_dataset,\n",
        "    epochs=3  # You can reduce or increase as needed\n",
        ")\n",
        "\n",
        "\n",
        "# Predict on dev set\n",
        "dev_preds = model.predict(dev_dataset).logits\n",
        "dev_pred_labels = np.argmax(dev_preds, axis=1)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Classification Report (Dev):\")\n",
        "print(classification_report(y_dev, dev_pred_labels, target_names=label_encoder.classes_))\n",
        "\n",
        "# Predict on test set\n",
        "test_preds = model.predict(test_dataset).logits\n",
        "test_pred_labels = np.argmax(test_preds, axis=1)\n",
        "test_labels_str = label_encoder.inverse_transform(test_pred_labels)\n",
        "\n",
        "# Save test predictions\n",
        "output_df = pd.DataFrame({\n",
        "    'id': test_df['id'],\n",
        "    'predicted_label': test_labels_str\n",
        "})\n",
        "output_df.to_csv('/content/test_predictions_mbert.csv', index=False)\n",
        "print(\"Predictions saved to test_predictions_mbert.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}